# Behaviorial Cloning Project

## Project Submission
Project submission consists of the following files:
* `model.py` defines model structure and executes training
* `drive.py` controls simulated vehicle in autonomous mode
* `bc_config.py` modifies behavior of model.py and drive.py
* `submission.h5` is the submission model and can be found [here](https://drive.google.com/open?id=1ft2uEHIyPrEgmz6umV8qrMc72mt58056)
* `README.md` the document you are reading

## Data Collection and Image Preprocessing
The model was trained on a collection of 64,560 examples.  Each example consisted of three images produced by the three simulated forward-facing cameras.  For the left and right offset cameras, the corresponding steering angle is adjusted by a constant approximation as described in the Udacity lecture video.  This approximation value was treated as a hyperparameter and adjusted empirically during multiple validation runs.  

Training images were generated by driving the simulated car around the track in forward and reverse directions and applying a random horizontal flip.  Driving was conducted using a notebook touchscreen to improve the steering input granularity over coarser keyboard steering.

Before the first convolutional layer, images were subjected to two preprocessing steps.  First, the images were normalized.  Normalization was accomplished by transforming the pixel values to their z-score within the batch.  Second, the top 33% of the input images were cropped.  This was done to improve model performance by removing information that should have no bearing on the steering image.

## Model Architecture and Training
The model employed the NVIDIA DAVE2 architecture modified only by the inclusion of dropout layers set to a rate of 20% after the first three fully-connected layers.  No regularization technique is described in the [source paper](http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf).  The final architecture is described in the following table:

| Layer Type        | Activation Volume |
| ----------------- | ----------------- |
| (Input)           | 320x160x3         |
| conv5-24          | 158x78x24         |
| conv5-36          | 77x37x36          |
| conv5-48          | 37x17x48          |
| conv3-64          | 35x15x64          |
| conv3-64          | 33x13x64          |
| FC-1164           | 1164              |
| Dropout (20%)     | 1164              |
| FC-100            | 100               |
| Dropout (20%)     | 100               |
| FC-50             | 50                |
| Dropout (20%)     | 50                |
| FC-10             | 10                |
| FC-1              | 1                 |

Training was accomplished with five epochs of training using an Adam Optimizer with the default learning rate of 1.0.  Training took approximately an hour on an Amazon AWS GPU instance.  10% of the data was reserved for validation and achieved a stead mean squared error (MSE) of 0.002.

Finally, during real-time driving, the resulting steering angle is adjusted by a fixed gain value.  This value is treated as a hyperparameter and adjusted empircally.  A value of 2.0 was found to improve the vehicle's agressiveness in seeking the road center.  I speculate that more training examples of off-center corrections would make a steering gain adjustment unnecessary.  However, the gain was a quick way to correct the dampended response the system demonstrated without it.  A video of model performance can be viewed at the YouTube video linked by the image below:

[![Submission Video](http://img.youtube.com/vi/SNMrS9EIvew/0.jpg)](http://www.youtube.com/watch?v=SNMrS9EIvew)
